{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "#from torchvision import models\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.ones((3,28,28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xray_classification = np.load('../data/npy_files/xray classification.npy', allow_pickle=True)\n",
    "xray_classification = xray_classification.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_classification = np.load('../data/npy_files/pixel classification.npy', allow_pickle=True)\n",
    "pixel_classification = pixel_classification.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct datasets of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dic = {}\n",
    "for key in ['domain 1', 'domain 2', 'domain 3']:\n",
    "    x, y = xray_classification[key].shape\n",
    "    dataset_dic[key] = []\n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            temp_vec = np.append(xray_classification[key][i][j], pixel_classification[key][i][j])\n",
    "            dataset_dic[key].append(temp_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d1 = np.array(dataset_dic['domain 1'])\n",
    "y_d1 = np.ones(x_d1.shape[0])\n",
    "\n",
    "x_d2 = np.array(dataset_dic['domain 2'])\n",
    "y_d2 = np.ones(x_d2.shape[0]) * 2\n",
    "\n",
    "x_d3 = np.array(dataset_dic['domain 3'])\n",
    "y_d3 = np.ones(x_d3.shape[0]) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.vstack((x_d1, x_d2, x_d3))\n",
    "data_y = np.hstack((y_d1, y_d2, y_d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#permutate the datasets \n",
    "indx = np.random.permutation(len(data_y))\n",
    "data_x = data_x[indx]\n",
    "data_y = data_y[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 10000\n",
    "testing_size = 1000\n",
    "\n",
    "training_x = data_x[0:training_size]\n",
    "training_y = data_y[0:training_size]\n",
    "\n",
    "testing_x = data_x[training_size:training_size + testing_size]\n",
    "testing_y = data_y[training_size:training_size + testing_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sample(n_way, n_support, n_query, datax, datay, test=False):\n",
    "    \"\"\"\n",
    "    Cite: https://github.com/cnielly/prototypical-networks-omniglot\n",
    "    \"\"\" \n",
    "    sample = []\n",
    "    # K is a random ordering of the domains (1,2,3), chooses from unique values\n",
    "    # of datay which is a number corresponding to the domain, n_way just says \n",
    "    # choose 3 from 1,2,3 without replacement\n",
    "    K = np.random.choice(np.unique(datay), n_way, replace=False)\n",
    "    for cls in K: # cls will == 1, 2, and 3 in some order\n",
    "        datax_cls = datax[datay == cls] # Selects every data point associated with a given domain\n",
    "        perm = np.random.permutation(datax_cls) # Randomizes\n",
    "        sample_cls = perm[:(n_support+n_query)] # Selects a small set of the domain \n",
    "        sample.append(sample_cls) # Adds to the sample\n",
    "    sample = np.array(sample).astype('float')\n",
    "    sample = torch.from_numpy(sample).float()\n",
    "    return ({\n",
    "      'data_vector': sample,\n",
    "      'n_way': n_way,\n",
    "      'n_support': n_support,\n",
    "      'n_query': n_query\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters for few-shot learning\n",
    "n_way = 3\n",
    "n_support = 10\n",
    "n_query = 20\n",
    "N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 1971])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = extract_sample(n_way, n_support, n_query, training_x, training_y)\n",
    "sample['data_vector'].shape\n",
    "# Sample is a dictionary containing the data vector (with the samples),\n",
    "# the number of domains, the number of support and query samples\n",
    "# So the data vector has 3 groups (each one corresponding to a domain) made up\n",
    "# of 30 samples each, each sample is the sampled spectrum with the brightness \n",
    "# of the pixel appended to the very end\n",
    "# And it is all a torch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the embedding\n",
    "\n",
    "def load_protonet_conv(x_dim, hid_dim, z_dim):\n",
    "\n",
    "    \"\"\"\n",
    "    Cite: https://github.com/cnielly/prototypical-networks-omniglot\n",
    "    \n",
    "    Loads the prototypical network model\n",
    "    Arg:\n",
    "      x_dim (tuple): dimension of input data\n",
    "      hid_dim (int): dimension of hidden layers\n",
    "      z_dim (int): dimension of output\n",
    "    Returns:\n",
    "      Model (Class ProtoNet)\n",
    "      \"\"\"\n",
    "    encoder = nn.Sequential(\n",
    "                    nn.Linear(x_dim[-1], hid_dim),\n",
    "                    nn.Linear(hid_dim, hid_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hid_dim, hid_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hid_dim, z_dim))\n",
    "\n",
    "    return ProtoNet(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    \"\"\"\n",
    "    Cite: https://github.com/cnielly/prototypical-networks-omniglot\n",
    "    Computes euclidean distance btw x and y\n",
    "    Args:\n",
    "      x (torch.Tensor): shape (n, d). n usually n_way*n_query\n",
    "      y (torch.Tensor): shape (m, d). m usually n_way\n",
    "    Returns:\n",
    "      torch.Tensor: shape(n, m). For each query, the distances to each centroid\n",
    "    \"\"\"\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    assert d == y.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cite: https://github.com/cnielly/prototypical-networks-omniglot\n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(ProtoNet, self).__init__()\n",
    "        self.encoder = encoder#.cuda()\n",
    "\n",
    "    def set_forward_loss(self, sample):\n",
    "        sample_vec = sample['data_vector']\n",
    "        n_way, n_support, n_query = sample['n_way'], sample['n_support'], sample['n_query']\n",
    "        x_support = sample_vec[:, :n_support]\n",
    "        x_query = sample_vec[:, n_support:]\n",
    "        \n",
    "        target_inds = torch.arange(0, n_way).view(n_way, 1, 1).expand(n_way, n_query, 1).long()\n",
    "        target_inds = torch.autograd.Variable(target_inds, requires_grad=False)\n",
    "        target_inds = target_inds#.cuda()\n",
    "        \n",
    "        x_s = x_support.contiguous().view(n_way * n_support, *x_support.size()[2:])\n",
    "        x_q = x_query.contiguous().view(n_way * n_query, *x_query.size()[2:])\n",
    "        \n",
    "        x = torch.cat([x_s,x_q], 0)\n",
    "        z = self.encoder(x.float())\n",
    "        z_dim = z.size(-1)\n",
    "        \n",
    "        z_proto = z[:n_way*n_support].view(n_way, n_support, z_dim).mean(1)\n",
    "        z_query = z[n_way*n_support:]\n",
    "        \n",
    "        #compute distances\n",
    "        dists = euclidean_dist(z_query, z_proto)\n",
    "\n",
    "        #compute probabilities\n",
    "        log_p_y = F.log_softmax(-dists, dim=1).view(n_way, n_query, -1)\n",
    "\n",
    "        loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "        _, y_hat = log_p_y.max(2)\n",
    "        acc_val = torch.eq(y_hat, target_inds.squeeze()).float().mean()\n",
    "        return loss_val, {\n",
    "            'loss': loss_val.item(),\n",
    "            'acc': acc_val.item(),\n",
    "            'y_hat': y_hat\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training \n",
    "def train(model, optimizer, n_way, n_support, n_query, max_epoch, epoch_size):\n",
    "    \"\"\"\n",
    "    Cite: https://github.com/cnielly/prototypical-networks-omniglot\n",
    "    Trains the protonet\n",
    "    Args:\n",
    "      model\n",
    "      optimizer\n",
    "      n_way (int): number of classes in a classification task\n",
    "      n_support (int): number of labeled examples per class in the support set\n",
    "      n_query (int): number of labeled examples per class in the query set\n",
    "      max_epoch (int): max epochs to train on\n",
    "      epoch_size (int): episodes per epoch\n",
    "    \"\"\"\n",
    "    #divide the learning rate by 2 at each epoch, as suggested in paper\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
    "    epoch = 0 \n",
    "    stop = False #status to know when to stop\n",
    "    \n",
    "    print(\"---------START training-------------\")\n",
    "    \n",
    "    while epoch < max_epoch and not stop:\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for episode in tnrange(epoch_size, desc=\"Epoch {:d} train\".format(epoch+1)):\n",
    "            if episode% 100 == 0:\n",
    "                print(\"=\", end='')\n",
    "            sample = extract_sample(n_way, n_support, n_query, training_x, training_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss, output = model.set_forward_loss(sample)\n",
    "            running_loss += output['loss']\n",
    "            running_acc += output['acc']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss = running_loss / epoch_size\n",
    "        epoch_acc = running_acc / epoch_size\n",
    "        epoch += 1\n",
    "        print('Epoch {:d} -- Loss: {:.4f} Acc: {:.4f}'.format(epoch,epoch_loss, epoch_acc))\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------START training-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6550b1aefb1e4adaa4940dd4f7e127de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch 1 -- Loss: 0.2527 Acc: 0.8940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7555a7652be84c0a846e82392e165ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch 2 -- Loss: 0.1690 Acc: 0.9268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67d9192773443b98248f62a35292c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch 3 -- Loss: 0.1501 Acc: 0.9351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72e0d2513a64320a203457b22601b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch 4 -- Loss: 0.1394 Acc: 0.9413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ece091db7f74745b4edb96facccff44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch 5 -- Loss: 0.1309 Acc: 0.9444\n"
     ]
    }
   ],
   "source": [
    "x_dim = sample['data_vector'].shape\n",
    "hid_dim = 60\n",
    "z_dim = 40\n",
    "\n",
    "model = load_protonet_conv(\n",
    "    x_dim,\n",
    "    hid_dim,\n",
    "    z_dim)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "max_epoch = 5\n",
    "epoch_size = 1000\n",
    "\n",
    "train(model, optimizer, n_way, n_support, n_query, max_epoch, epoch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbdb23fe7ff43e7b09d44a1e0c5fc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results -- Loss: 0.2337 Acc: 0.9122\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_x, test_y, n_way, n_support, n_query, test_episode):\n",
    "    \"\"\"\n",
    "    Cite: https://github.com/cnielly/prototypical-networks-omniglot\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for episode in tnrange(test_episode):\n",
    "        sample = extract_sample(n_way, n_support, n_query, testing_x, testing_y)\n",
    "        loss, output = model.set_forward_loss(sample)\n",
    "        running_loss += output['loss']\n",
    "        running_acc += output['acc']\n",
    "    avg_loss = running_loss / test_episode\n",
    "    avg_acc = running_acc / test_episode\n",
    "    print('Test results -- Loss: {:.4f} Acc: {:.4f}'.format(avg_loss, avg_acc))\n",
    "\n",
    "n_way = 3\n",
    "n_support = 5\n",
    "n_query = 10\n",
    "\n",
    "test_x = testing_x\n",
    "test_y = testing_y\n",
    "\n",
    "test_episode = 2000\n",
    "\n",
    "test(model, test_x, test_y, n_way, n_support, n_query, test_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for region one 0.9499800000000002\n",
      "for region two 0.9490099999999997\n",
      "for region three 0.9440400000000001\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "acc_1 = 0\n",
    "acc_2 = 0\n",
    "acc_3 = 0\n",
    "for _ in range(N):\n",
    "    sample = extract_sample(n_way, n_support, 1000, training_x, training_y)\n",
    "    loss, output = model.set_forward_loss(sample)\n",
    "    output['y_hat'][0]\n",
    "    acc_1 += np.count_nonzero(output['y_hat'][0] == 0)/1000\n",
    "    acc_2 += np.count_nonzero(output['y_hat'][1] == 1)/1000\n",
    "    acc_3 += np.count_nonzero(output['y_hat'][2] == 2)/1000\n",
    "#region one classification accuracy\n",
    "print(\"for region one\", acc_1/N)\n",
    "print(\"for region two\", acc_2/N)\n",
    "print(\"for region three\", acc_3/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output\n",
    "\n",
    "- n_way = 3, n_support = 5, n_query = 10: training accuracy = 0.9372; testing accuracy = 0.9012;\n",
    "- n_way = 3, n_support = 5, n_query = 5: training accuracy = 0.9373; testing accuracy = 0.9011;\n",
    "- n_way = 3, n_support = 5, n_query = 20: training accuracy = 0.949; testing accuracy = 0.897;\n",
    "- n_way = 3, n_support = 20, n_query = 10: training accuracy = 0.9470; testing accuracy = 0.8967;\n",
    "- n_way = 3, n_support = 10, n_query = 20: training accuracy = 0.9506; testing accuracy = 0.8917;\n",
    "\n",
    "- n_way = 2, n_support = 5, n_query = 20: training accuracy = 0.9681; testing accuracy = 0.9051;\n",
    "- n_way = 2, n_support = 5, n_query = 10: training accuracy = 0.9665; testing accuracy = 0.9015;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_env",
   "language": "python",
   "name": "multimodal_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
